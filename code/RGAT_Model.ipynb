{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load results from BERT Embeddings Generatation with Punc.ipynb and BERT Embeddings Generatation without Punc.ipynb\n",
    "'''\n",
    "\n",
    "token_lst = pickle.load(open('token_lst_wto_padding.pkl', \"rb\")) # tokens of every sentence without padding\n",
    "bert_outputs_lst = pickle.load(open('bert_outputs.pkl', \"rb\")) # list of outputs of bert for every sentence\n",
    "offsets_lst = pickle.load(open('offsets_lst.pkl', \"rb\"))\n",
    "\n",
    "test_token_lst = pickle.load(open('test_token_lst_wto_padding.pkl', \"rb\")) # tokens of every sentence without padding\n",
    "test_bert_outputs_lst = pickle.load(open('test_bert_outputs.pkl', \"rb\")) # list of outputs of bert for every sentence\n",
    "test_offsets_lst = pickle.load(open('test_offsets_lst.pkl', \"rb\"))\n",
    "\n",
    "others_bert_outputs = pickle.load(open('others_bert_outputs.pkl', \"rb\"))\n",
    "test_others_bert_outputs  = pickle.load(open('test_others_bert_outputs.pkl', \"rb\"))\n",
    "\n",
    "\n",
    "train_df = pd.concat([\n",
    "    pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\"),\n",
    "    pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\n",
    "], axis=0)\n",
    "\n",
    "test_df = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GATNE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class RGATModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_size, embedding_u_size, edge_type_count, dim_a\n",
    "    ):\n",
    "        super(RGATModel, self).__init__()\n",
    "        self.embedding_size = embedding_size  # 每个节点输出的embedding_size\n",
    "        self.embedding_u_size = embedding_u_size  # 节点作为邻居初始化size\n",
    "        self.edge_type_count = edge_type_count  # 类别数量\n",
    "        self.dim_a = dim_a  # 中间隐层特征数量\n",
    "\n",
    "        feature_dim = 1024\n",
    "        self.embed_trans = nn.Parameter(torch.FloatTensor(feature_dim, embedding_size)) \n",
    "        self.u_embed_trans = nn.Parameter(torch.FloatTensor(edge_type_count, feature_dim, embedding_u_size)) \n",
    "\n",
    "        self.trans_weights = nn.Parameter(\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, embedding_size)\n",
    "        )\n",
    "        self.trans_weights_s1 = nn.Parameter(\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, dim_a)\n",
    "        )\n",
    "        self.trans_weights_s2 = nn.Parameter(torch.FloatTensor(edge_type_count, dim_a, 1)) \n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.u_embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        \n",
    "        self.trans_weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s1.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s2.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "\n",
    "    def forward(self,features_lst, train_types_lst, node_neigh_lst):\n",
    "        rst_hidden = []\n",
    "        for features,train_types,node_neigh in zip(features_lst,train_types_lst,node_neigh_lst):\n",
    "            train_types = train_types.cuda()\n",
    "            node_neigh = node_neigh.cuda()\n",
    "\n",
    "            num_nodes = int(features.shape[0]/3)\n",
    "            node_embed = torch.matmul(features, self.embed_trans)\n",
    "            node_embed_neighbors = torch.einsum('bijk,akm->bijam', features[node_neigh], self.u_embed_trans)\n",
    "\n",
    "            node_embed_tmp = torch.cat(  # Aggregate the neighbor information around each category\n",
    "                [\n",
    "                    node_embed_neighbors[:, i, :, i, :].unsqueeze(1)\n",
    "                    for i in range(self.edge_type_count)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            node_type_embed = torch.sum(node_embed_tmp, dim=2).repeat(3,1,1)  # Sum the neighbor information\n",
    "\n",
    "            trans_w = self.trans_weights[train_types]\n",
    "            trans_w_s1 = self.trans_weights_s1[train_types]\n",
    "            trans_w_s2 = self.trans_weights_s2[train_types]\n",
    "\n",
    "            attention = F.softmax(\n",
    "                torch.matmul(\n",
    "                    torch.tanh(torch.matmul(node_type_embed, trans_w_s1)), trans_w_s2\n",
    "                ).squeeze(2),\n",
    "                dim=1,\n",
    "            ).unsqueeze(1)\n",
    "            node_type_embed = torch.matmul(attention, node_type_embed)  #对node_type_embed做attention求和\n",
    "            node_embed = node_embed + torch.matmul(node_type_embed, trans_w).squeeze(1)\n",
    "            last_node_embed = F.normalize(node_embed, dim=1)\n",
    "            \n",
    "            # concat\n",
    "            last_node_embed = torch.cat((last_node_embed[:num_nodes],last_node_embed[num_nodes:2*num_nodes],last_node_embed[2*num_nodes:]),dim =1)\n",
    "            rst_hidden.append(last_node_embed)\n",
    "        return rst_hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the Main Model (RGAT + FFNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"The MLP submodule\"\"\"\n",
    "    def __init__(self, rgat_out_size: int, bert_out_size: int):\n",
    "        super().__init__()\n",
    "        self.bert_out_size = bert_out_size\n",
    "        self.rgat_out_size = rgat_out_size\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(bert_out_size * 3 + rgat_out_size * 3),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(bert_out_size * 3 + rgat_out_size * 3, 256),    \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 3),\n",
    "        )\n",
    "        for i, module in enumerate(self.fc):\n",
    "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                if getattr(module, \"weight_v\", None) is not None:\n",
    "                    nn.init.uniform_(module.weight_g, 0, 1)\n",
    "                    nn.init.kaiming_normal_(module.weight_v)\n",
    "                    assert model[i].weight_g is not None\n",
    "                else:\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, rgat_outputs, offsets_rgat, bert_embeddings):\n",
    "        \n",
    "        rgat_extracted_outputs = [rgat_outputs[i].unsqueeze(0).gather(1, offsets_rgat[i].unsqueeze(0).unsqueeze(2)\n",
    "                                       .expand(-1, -1, rgat_outputs[i].unsqueeze(0).size(2)))\n",
    "                                 .view(rgat_outputs[i].unsqueeze(0).size(0), -1) for i in range(len(rgat_outputs))]\n",
    "        \n",
    "        rgat_extracted_outputs = torch.stack(rgat_extracted_outputs, dim=0).squeeze()\n",
    "        \n",
    "        embeddings = torch.cat((rgat_extracted_outputs, bert_embeddings), 1) \n",
    "        \n",
    "        return self.fc(embeddings)\n",
    "\n",
    "\n",
    "class BERT_Head(nn.Module):\n",
    "    def __init__(self, bert_hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(bert_hidden_size * 3),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(bert_hidden_size * 3, 512 * 3),   \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        for i, module in enumerate(self.fc):\n",
    "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                if getattr(module, \"weight_v\", None) is not None:\n",
    "                    nn.init.uniform_(module.weight_g, 0, 1)\n",
    "                    nn.init.kaiming_normal_(module.weight_v)\n",
    "                    assert model[i].weight_g is not None\n",
    "                else:\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, bert_embeddings):\n",
    "        outputs = self.fc(bert_embeddings.view(bert_embeddings.shape[0],-1))\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "class GPRModel(nn.Module):\n",
    "    \"\"\"The main model.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.RGAT =  RGATModel(256,10,3,20)\n",
    "        self.BERThead = BERT_Head(1024) # bert output size\n",
    "        self.head = Head(256*3, 512)  # rgat output   berthead output\n",
    "    \n",
    "    \n",
    "    def forward(self, offsets_bert, offsets_rgat, bert_embeddings, features,train_types,neighbors):\n",
    "        rgat_outputs = self.RGAT(features,train_types,neighbors)\n",
    "        bert_head_outputs = self.BERThead(bert_embeddings)\n",
    "        head_outputs = self.head(rgat_outputs, offsets_rgat, bert_head_outputs)\n",
    "        return head_outputs            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate All Syntactic Graphs with BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = spacy.load('en_core_web_lg') \n",
    "# An error may be reported that the model cannot be found and needs to be executed on the terminal:\n",
    "# python -m spacy download en_core_web_lg\n",
    "\n",
    "BERT_MODEL = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[THISISA]\", \"[THISISB]\", \"[THISISP]\"))\n",
    "\n",
    "tokenizer.vocab[\"[THISISA]\"] = -1\n",
    "tokenizer.vocab[\"[THISISB]\"] = -1\n",
    "tokenizer.vocab[\"[THISISP]\"] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n",
      "/root/miniconda3/lib/python3.8/site-packages/dgl/heterograph.py:3432: DGLWarning: DGLGraph.in_degree is deprecated. Please use DGLGraph.in_degrees\n",
      "  dgl_warning(\"DGLGraph.in_degree is deprecated. Please use DGLGraph.in_degrees\")\n"
     ]
    }
   ],
   "source": [
    "def is_target(i, target_offset_list):\n",
    "    return i in target_offset_list\n",
    "\n",
    "def transfer_n_e(nodes, edges):\n",
    "\n",
    "    num_nodes = len(nodes)\n",
    "    new_edges = []\n",
    "    for e1, e2 in edges:\n",
    "        new_edges.append( [nodes[e1], nodes[e2]] ) \n",
    "    return num_nodes, new_edges\n",
    "\n",
    "def gen_edge_by_type(tran_edges,edge_type):\n",
    "    edge_data_by_type = dict()\n",
    "    for i in range(len(edge_type)):\n",
    "        if str(edge_type[i]) not in edge_data_by_type:\n",
    "            edge_data_by_type[str(edge_type[i])] = list()\n",
    "        edge_data_by_type[str(edge_type[i])].append(tran_edges[i])\n",
    "    return edge_data_by_type\n",
    "\n",
    "def generate_neighbors(network_data, num_nodes , edge_types=['0','1','2'], neighbor_samples=4):\n",
    "    edge_type_count = len(edge_types)\n",
    "    neighbors = [[[] for __ in range(edge_type_count)] for _ in range(num_nodes)]\n",
    "    for r in range(edge_type_count):\n",
    "        # print('Generating neighbors for layer', r)\n",
    "        g = network_data[edge_types[r]]  # Nodes involved in each type\n",
    "        for (x, y) in g:\n",
    "            ix = x  # The index that x corresponds to\n",
    "            iy = y  # The index that y corresponds to\n",
    "            neighbors[ix][r].append(iy)  # The neighbor information\n",
    "            neighbors[iy][r].append(ix)\n",
    "        for i in range(num_nodes):  # Sample a fixed number of neighbors\n",
    "            if len(neighbors[i][r]) == 0:  \n",
    "                neighbors[i][r] = [i] * neighbor_samples\n",
    "            elif len(neighbors[i][r]) < neighbor_samples:  \n",
    "                neighbors[i][r].extend(list(np.random.choice(neighbors[i][r], size=neighbor_samples-len(neighbors[i][r]))))\n",
    "            elif len(neighbors[i][r]) > neighbor_samples:  \n",
    "                neighbors[i][r] = list(np.random.choice(neighbors[i][r], size=neighbor_samples))\n",
    "    return neighbors  # The neighbor sampling result of each node\n",
    "\n",
    "\n",
    "all_train_types = []\n",
    "all_neighbors = []\n",
    "all_features = []\n",
    "\n",
    "all_graphs = []\n",
    "rgat_offsets = []\n",
    "for i, sent_token in enumerate(token_lst):\n",
    "    sent_token = token_lst[i]\n",
    "\n",
    "    sent = ' '.join([re.sub(\"[#]\",\"\",token)   for token in tokenizer.convert_ids_to_tokens(sent_token[1:-1])])\n",
    "    doc = parser(sent)\n",
    "    parse_rst = doc.to_json()\n",
    "\n",
    "    target_offset_list = [item - 1 for item in offsets_lst[i]]\n",
    "    \n",
    "    nodes = collections.OrderedDict()\n",
    "    edges = []\n",
    "    edge_type = []\n",
    "    for i_word, word in enumerate(parse_rst['tokens']):\n",
    "        if not (is_target(i_word, target_offset_list) or is_target(word['head'], target_offset_list)):\n",
    "            continue\n",
    "\n",
    "        if i_word not in nodes:\n",
    "            nodes[i_word] = len(nodes) \n",
    "            edges.append( [i_word, i_word] )\n",
    "            edge_type.append(0)\n",
    "        if word['head'] not in nodes:\n",
    "            nodes[word['head']] = len(nodes) \n",
    "            edges.append( [word['head'], word['head']] )\n",
    "            edge_type.append(0)\n",
    "\n",
    "        if word['dep'] != 'ROOT':\n",
    "                edges.append( [word['head'], word['id']] )\n",
    "                edge_type.append(1)\n",
    "                edges.append( [word['id'], word['head']] )\n",
    "                edge_type.append(2)\n",
    "\n",
    "    num_nodes, tran_edges = transfer_n_e(nodes, edges)\n",
    "    \n",
    "    edge_data_by_type = gen_edge_by_type(tran_edges,edge_type)\n",
    "    neighbors = generate_neighbors(edge_data_by_type, num_nodes)\n",
    "    train_types = [0]*num_nodes +[1]*num_nodes+[2]*num_nodes\n",
    "    \n",
    "    all_train_types.append(torch.from_numpy(np.array(train_types,dtype = int)))\n",
    "    all_neighbors.append(torch.from_numpy(np.array(neighbors)))\n",
    "    \n",
    "    rgat_offset = [nodes[offset] for offset in target_offset_list]\n",
    "    rgat_offsets.append(rgat_offset)\n",
    "    \n",
    "    G = dgl.DGLGraph()\n",
    "    G = G.to('cuda:0')  \n",
    "    G.add_nodes(num_nodes)\n",
    "    G.add_edges(list(zip(*tran_edges))[0],list(zip(*tran_edges))[1]) \n",
    "\n",
    "    for i_word, word in enumerate(parse_rst['tokens']):\n",
    "        if not (is_target(i_word, target_offset_list) or is_target(word['head'], target_offset_list)):\n",
    "            continue\n",
    "        if is_target(i_word, target_offset_list): \n",
    "            G.nodes[ [ nodes[i_word] ]].data['h'] = others_bert_outputs[i][0][target_offset_list.index(i_word)].unsqueeze(0).cuda()\n",
    "        else:\n",
    "            G.nodes[ [ nodes[i_word] ]].data['h'] = bert_outputs_lst[i][0][i_word + 1].unsqueeze(0).cuda()\n",
    "        if is_target(word['head'], target_offset_list):\n",
    "            G.nodes[ [ nodes[word['head']] ]].data['h'] = others_bert_outputs[i][0][target_offset_list.index(word['head'])].unsqueeze(0).cuda()\n",
    "        else:   \n",
    "            G.nodes[ [ nodes[word['head']] ]].data['h'] = bert_outputs_lst[i][0][word['head'] + 1].unsqueeze(0).cuda()\n",
    "\n",
    "    edge_norm = []\n",
    "    for e1, e2 in tran_edges:\n",
    "        if e1 == e2:\n",
    "            edge_norm.append(1)\n",
    "        else:\n",
    "            edge_norm.append( 1 / (G.in_degree(e2) - 1 ) )\n",
    "\n",
    "\n",
    "    edge_type = torch.from_numpy(np.array(edge_type)).cuda()\n",
    "    edge_norm = torch.from_numpy(np.array(edge_norm)).unsqueeze(1).float().cuda()\n",
    "\n",
    "    G.edata.update({'rel_type': edge_type,})\n",
    "    G.edata.update({'norm': edge_norm})\n",
    "    all_graphs.append(G)\n",
    "    all_features.append(G.ndata['h'].repeat(3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_train_types = []\n",
    "test_all_neighbors = []\n",
    "test_all_features = []\n",
    "\n",
    "test_all_graphs = []\n",
    "test_rgat_offsets = []\n",
    "for i, sent_token in enumerate(test_token_lst):\n",
    "    sent_token = test_token_lst[i]\n",
    "\n",
    "    sent = ' '.join([re.sub(\"[#]\",\"\",token)   for token in tokenizer.convert_ids_to_tokens(sent_token[1:-1])])\n",
    "    doc = parser(sent)\n",
    "    parse_rst = doc.to_json()\n",
    "\n",
    "    target_offset_list = [item - 1 for item in test_offsets_lst[i]]\n",
    "    \n",
    "    nodes = collections.OrderedDict()\n",
    "    edges = []\n",
    "    edge_type = []\n",
    "    for i_word, word in enumerate(parse_rst['tokens']):\n",
    "        if not (is_target(i_word, target_offset_list) or is_target(word['head'], target_offset_list)):\n",
    "            continue\n",
    "\n",
    "        if i_word not in nodes:\n",
    "            nodes[i_word] = len(nodes) \n",
    "            edges.append( [i_word, i_word] )\n",
    "            edge_type.append(0)\n",
    "        if word['head'] not in nodes:\n",
    "            nodes[word['head']] = len(nodes) \n",
    "            edges.append( [word['head'], word['head']] )\n",
    "            edge_type.append(0)\n",
    "\n",
    "        if word['dep'] != 'ROOT':\n",
    "                edges.append( [word['head'], word['id']] )\n",
    "                edge_type.append(1)\n",
    "                edges.append( [word['id'], word['head']] )\n",
    "                edge_type.append(2)\n",
    "\n",
    "    num_nodes, tran_edges = transfer_n_e(nodes, edges)\n",
    "    \n",
    "    edge_data_by_type = gen_edge_by_type(tran_edges,edge_type)\n",
    "    neighbors = generate_neighbors(edge_data_by_type, num_nodes)\n",
    "    train_types = [0]*num_nodes +[1]*num_nodes+[2]*num_nodes\n",
    "    \n",
    "    test_all_train_types.append(torch.from_numpy(np.array(train_types,dtype = int)))\n",
    "    test_all_neighbors.append(torch.from_numpy(np.array(neighbors)))\n",
    "    \n",
    "    test_rgat_offset = [nodes[offset] for offset in target_offset_list]\n",
    "    test_rgat_offsets.append(test_rgat_offset)\n",
    "    \n",
    "    G = dgl.DGLGraph()\n",
    "    G = G.to('cuda:0')\n",
    "    G.add_nodes(num_nodes)\n",
    "    G.add_edges(list(zip(*tran_edges))[0],list(zip(*tran_edges))[1]) \n",
    "\n",
    "    for i_word, word in enumerate(parse_rst['tokens']):\n",
    "        if not (is_target(i_word, target_offset_list) or is_target(word['head'], target_offset_list)):\n",
    "            continue\n",
    "        \n",
    "        if is_target(i_word, target_offset_list): \n",
    "            G.nodes[ [ nodes[i_word] ]].data['h'] = test_others_bert_outputs[i][0][target_offset_list.index(i_word)].unsqueeze(0).cuda()\n",
    "        else:\n",
    "            G.nodes[ [ nodes[i_word] ]].data['h'] = test_bert_outputs_lst[i][0][i_word + 1].unsqueeze(0).cuda()\n",
    "        if is_target(word['head'], target_offset_list):\n",
    "            G.nodes[ [ nodes[word['head']] ]].data['h'] = test_others_bert_outputs[i][0][target_offset_list.index(word['head'])].unsqueeze(0).cuda()\n",
    "        else:   \n",
    "            G.nodes[ [ nodes[word['head']] ]].data['h'] = test_bert_outputs_lst[i][0][word['head'] + 1].unsqueeze(0).cuda()\n",
    "\n",
    "    edge_norm = []\n",
    "    for e1, e2 in tran_edges:\n",
    "        if e1 == e2:\n",
    "            edge_norm.append(1)\n",
    "        else:\n",
    "            edge_norm.append( 1 / (G.in_degree(e2) - 1 ) )\n",
    "\n",
    "\n",
    "    edge_type = torch.from_numpy(np.array(edge_type)).cuda()\n",
    "    edge_norm = torch.from_numpy(np.array(edge_norm)).unsqueeze(1).float().cuda()\n",
    "\n",
    "    G.edata.update({'rel_type': edge_type,})\n",
    "    G.edata.update({'norm': edge_norm})\n",
    "    test_all_graphs.append(G)\n",
    "    test_all_features.append(G.ndata['h'].repeat(3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Dataloader and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRDataset(Dataset):\n",
    "    def __init__(self, original_df, graphs, bert_offsets, rgat_offsets, bert_embeddings,features,train_types,neighbors):\n",
    "        \n",
    "\n",
    "        tmp = original_df[[\"A-coref\", \"B-coref\"]].copy()\n",
    "        tmp[\"Neither\"] = ~(original_df[\"A-coref\"] | original_df[\"B-coref\"])\n",
    "        self.y = tmp.values.astype(\"bool\")\n",
    "\n",
    "        self.graphs = graphs\n",
    "        self.bert_offsets = bert_offsets  # already +1\n",
    "        self.bert_embeddings = bert_embeddings  # include [CLS]\n",
    "        self.rgat_offsets = rgat_offsets\n",
    "        self.features = features\n",
    "        self.train_types = train_types\n",
    "        self.neighbors = neighbors\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.bert_offsets[idx], self.rgat_offsets[idx], self.bert_embeddings[idx], \\\n",
    "    self.y[idx],self.features[idx],self.train_types[idx],self.neighbors[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    \n",
    "    graphs, bert_offsets, rgat_offsets, bert_embeddings, labels,features,train_types,neighbors = map(list, zip(*samples))\n",
    "    \n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    offsets_bert = torch.stack([torch.LongTensor(x) for x in bert_offsets], dim=0)\n",
    "    offsets_rgat = torch.stack([torch.LongTensor(x) for x in rgat_offsets], dim=0)\n",
    "    \n",
    "    one_hot_labels = torch.stack([torch.from_numpy(x.astype(\"uint8\")) for x in labels], dim=0)\n",
    "    _, labels = one_hot_labels.max(dim=1)\n",
    "    \n",
    "    bert_embeddings = torch.stack(bert_embeddings, dim=0).squeeze()\n",
    "    \n",
    "    \n",
    "    return batched_graph, offsets_bert, offsets_rgat, bert_embeddings, labels,features,train_types,neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DataLoarder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GPRDataset(original_df = test_df, graphs = test_all_graphs, bert_offsets = test_offsets_lst, rgat_offsets = test_rgat_offsets, bert_embeddings = test_others_bert_outputs,features=test_all_features,train_types=test_all_train_types,neighbors=test_all_neighbors)\n",
    "# train_dataset = GPRDataset(original_df = train_df, graphs = all_graphs, bert_offsets = offsets_lst, rgat_offsets= rgat_offsets, bert_embeddings = others_bert_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloarder = DataLoader(\n",
    "#    train_dataset,\n",
    "#    collate_fn = collate,\n",
    "#    batch_size = 4,\n",
    "#    shuffle=True,\n",
    "# )\n",
    "\n",
    "test_dataloarder = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn = collate,\n",
    "    batch_size = 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_graph_to_cpu(g):\n",
    "    # nodes\n",
    "    g = g.to(\"cpu\")\n",
    "    labels = g.node_attr_schemes()\n",
    "    for l in labels.keys():\n",
    "        g.ndata[l] = g.ndata.pop(l).cpu()\n",
    "    # edges\n",
    "    labels = g.edge_attr_schemes()\n",
    "    for l in labels.keys():\n",
    "        g.edata[l] = g.edata.pop(l).cpu()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_value = 0.0001\n",
    "total_epoch = 100\n",
    "def adjust_learning_rate(optimizers, epoch):\n",
    "    # warm up\n",
    "    if epoch < 10:\n",
    "        lr_tmp = 0.00001\n",
    "    else:\n",
    "        lr_tmp = lr_value * pow((1 - 1.0 * epoch / 100), 0.9)\n",
    "    \n",
    "    if epoch > 36:\n",
    "        lr_tmp =  0.000015 * pow((1 - 1.0 * epoch / 100), 0.9)\n",
    "    \n",
    "    for optimizer in optimizers:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_tmp\n",
    "\n",
    "    return lr_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate labels\n",
    "tmp = train_df[[\"A-coref\", \"B-coref\"]].copy()\n",
    "tmp[\"Neither\"] = ~(train_df[\"A-coref\"] | train_df[\"B-coref\"])\n",
    "train_y = tmp.values.astype(\"bool\").argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 1\n",
      "====================\n",
      "Dataloader Success---------------------\n",
      "|                                                                                   |\n",
      "Epoch 0, val_loss 1.2307\n",
      "| >>>>>                                                                             |\n",
      "| >>>>>>>>>>                                                                        |\n",
      "| >>>>>>>>>>>>>>>                                                                   |\n",
      "| >>>>>>>>>>>>>>>>>>>>                                                              |\n",
      "Epoch 20, val_loss 0.6380\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>                                                         |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                    |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                               |\n",
      "Best val loss found:  0.5709718752924989\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                          |\n",
      "Epoch 40, val_loss 0.5864\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                     |\n",
      "Best val loss found:  0.5650455603694043\n",
      "Best val loss found:  0.5650010632911349\n",
      "Best val loss found:  0.5619122031016078\n",
      "Best val loss found:  0.5598748835727452\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                           |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                      |\n",
      "Epoch 60, val_loss 0.5710\n",
      "Best val loss found:  0.5593173817345282\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                 |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>            |\n",
      "Best val loss found:  0.5591763234962293\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>       |\n",
      "Best val loss found:  0.5581274347879538\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "Epoch 80, val_loss 0.5824\n",
      "Best val loss found:  0.5531673313277524\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "Best val loss found:  0.5516388012868602\n",
      "This fold, the best val loss is:  0.5516388012868602\n",
      "This fold, the test loss is:  0.5388422336354852\n",
      "====================\n",
      "Fold 2\n",
      "====================\n",
      "Dataloader Success---------------------\n",
      "|                                                                                   |\n",
      "Epoch 0, val_loss 1.2074\n",
      "| >>>>>                                                                             |\n",
      "| >>>>>>>>>>                                                                        |\n",
      "| >>>>>>>>>>>>>>>                                                                   |\n",
      "| >>>>>>>>>>>>>>>>>>>>                                                              |\n",
      "Epoch 20, val_loss 0.6621\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>                                                         |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                    |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                               |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                          |\n",
      "Epoch 40, val_loss 0.6367\n",
      "Best val loss found:  0.6211704630556145\n",
      "Best val loss found:  0.6150425794890256\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                     |\n",
      "Best val loss found:  0.6140895640946985\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                           |\n",
      "Best val loss found:  0.6133002303843575\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                      |\n",
      "Epoch 60, val_loss 0.6331\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                 |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>            |\n",
      "Best val loss found:  0.6100007140842395\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>       |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "Epoch 80, val_loss 0.6128\n",
      "Best val loss found:  0.6099041128546242\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "Best val loss found:  0.6078854833494841\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "This fold, the best val loss is:  0.6078854833494841\n",
      "This fold, the test loss is:  0.5301785775870085\n",
      "====================\n",
      "Fold 3\n",
      "====================\n",
      "Dataloader Success---------------------\n",
      "|                                                                                   |\n",
      "Epoch 0, val_loss 1.2257\n",
      "| >>>>>                                                                             |\n",
      "| >>>>>>>>>>                                                                        |\n",
      "| >>>>>>>>>>>>>>>                                                                   |\n",
      "| >>>>>>>>>>>>>>>>>>>>                                                              |\n",
      "Epoch 20, val_loss 0.6611\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>                                                         |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                    |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                               |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                          |\n",
      "Epoch 40, val_loss 0.6208\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                     |\n",
      "Best val loss found:  0.6152164957872251\n",
      "Best val loss found:  0.614001054877067\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                           |\n",
      "Best val loss found:  0.613382164055739\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                      |\n",
      "Epoch 60, val_loss 0.6247\n",
      "Best val loss found:  0.6125446619178222\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                 |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>            |\n",
      "Best val loss found:  0.6107120273740796\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>       |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "Epoch 80, val_loss 0.6293\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "Best val loss found:  0.6098576856216764\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "Best val loss found:  0.60976686387709\n",
      "This fold, the best val loss is:  0.60976686387709\n",
      "This fold, the test loss is:  0.5387321502715349\n",
      "====================\n",
      "Fold 4\n",
      "====================\n",
      "Dataloader Success---------------------\n",
      "|                                                                                   |\n",
      "Epoch 0, val_loss 1.3193\n",
      "| >>>>>                                                                             |\n",
      "| >>>>>>>>>>                                                                        |\n",
      "| >>>>>>>>>>>>>>>                                                                   |\n",
      "| >>>>>>>>>>>>>>>>>>>>                                                              |\n",
      "Epoch 20, val_loss 0.5706\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>                                                         |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                    |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                               |\n",
      "Best val loss found:  0.5353950703804328\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                          |\n",
      "Epoch 40, val_loss 0.5423\n",
      "Best val loss found:  0.5312808212044278\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                     |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                           |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                      |\n",
      "Epoch 60, val_loss 0.5309\n",
      "Best val loss found:  0.530942053632523\n",
      "Best val loss found:  0.5245389600049674\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                 |\n",
      "Best val loss found:  0.5227831432944149\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>            |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>       |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "Epoch 80, val_loss 0.5360\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "This fold, the best val loss is:  0.5227831432944149\n",
      "This fold, the test loss is:  0.5243286197967827\n",
      "====================\n",
      "Fold 5\n",
      "====================\n",
      "Dataloader Success---------------------\n",
      "|                                                                                   |\n",
      "Epoch 0, val_loss 1.2113\n",
      "| >>>>>                                                                             |\n",
      "| >>>>>>>>>>                                                                        |\n",
      "| >>>>>>>>>>>>>>>                                                                   |\n",
      "| >>>>>>>>>>>>>>>>>>>>                                                              |\n",
      "Epoch 20, val_loss 0.6169\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>                                                         |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                                    |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                               |\n",
      "Best val loss found:  0.5495183968204793\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                          |\n",
      "Epoch 40, val_loss 0.5614\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                     |\n",
      "Best val loss found:  0.5464360133055749\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                           |\n",
      "Best val loss found:  0.5444440135383993\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                      |\n",
      "Epoch 60, val_loss 0.5515\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                 |\n",
      "Best val loss found:  0.5434760322900323\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>            |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>       |\n",
      "Best val loss found:  0.5414003345782195\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "Epoch 80, val_loss 0.5587\n",
      "Best val loss found:  0.5394087882061315\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "| >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |\n",
      "This fold, the best val loss is:  0.5394087882061315\n",
      "This fold, the test loss is:  0.5442516324706376\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = 5)\n",
    "test_predict_lst = [] # the test output for every fold\n",
    "for train_index, test_index in kfold.split(train_df, train_y):\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"Fold {len(test_predict_lst) + 1}\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    val_dataset = GPRDataset(original_df = train_df.iloc[test_index], graphs = list(itemgetter(*test_index)(all_graphs)), bert_offsets = list(itemgetter(*test_index)(offsets_lst)), rgat_offsets= list(itemgetter(*test_index)(rgat_offsets)), bert_embeddings = list(itemgetter(*test_index)(others_bert_outputs)),features=list(itemgetter(*test_index)(all_features)),train_types=list(itemgetter(*test_index)(all_train_types)),neighbors=list(itemgetter(*test_index)(all_neighbors)))\n",
    "    \n",
    "    train_dataset = GPRDataset(original_df = train_df.iloc[train_index], graphs = list(itemgetter(*train_index)(all_graphs)), bert_offsets = list(itemgetter(*train_index)(offsets_lst)), rgat_offsets= list(itemgetter(*train_index)(rgat_offsets)), bert_embeddings = list(itemgetter(*train_index)(others_bert_outputs)),features=list(itemgetter(*train_index)(all_features)),train_types=list(itemgetter(*train_index)(all_train_types)),neighbors=list(itemgetter(*train_index)(all_neighbors)))\n",
    "    \n",
    "    train_dataloarder = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn = collate,\n",
    "    batch_size = 4,\n",
    "    shuffle=True,)\n",
    "    \n",
    "    val_dataloarder = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn = collate,\n",
    "    batch_size = 4,)\n",
    "\n",
    "    model = GPRModel().cuda()\n",
    "    loss_func = nn.CrossEntropyLoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_value)\n",
    "    reg_lambda = 0.035\n",
    "\n",
    "    print('Dataloader Success---------------------')\n",
    "    \n",
    "    best_val_loss = 11\n",
    "    for epoch in range(total_epoch):\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('|',\">\" * epoch,\" \"*(80-epoch),'|')\n",
    "        \n",
    "        lr = adjust_learning_rate([optimizer],epoch)\n",
    "        # print(\"Learning rate = %4f\\n\" % lr)\n",
    "        model.train()\n",
    "        for iter, (batched_graph, offsets_bert, offsets_rgat, bert_embeddings, labels,features,train_types,neighbors) in enumerate(train_dataloarder):\n",
    "            bert_embeddings = bert_embeddings.cuda()\n",
    "            labels = labels.cuda()\n",
    "            offsets_rgat = offsets_rgat.cuda()\n",
    "\n",
    "            prediction = model(offsets_bert, offsets_rgat, bert_embeddings,features,train_types,neighbors )\n",
    "\n",
    "            l2_reg = None\n",
    "            for w in model.RGAT.parameters():\n",
    "                if not l2_reg:\n",
    "                    l2_reg = w.norm(2)\n",
    "                else:\n",
    "                    l2_reg = l2_reg + w.norm(2)  \n",
    "            for w in model.head.parameters():\n",
    "                if not l2_reg:\n",
    "                    l2_reg = w.norm(2)\n",
    "                else:\n",
    "                    l2_reg = l2_reg + w.norm(2)   \n",
    "            for w in model.BERThead.parameters():\n",
    "                if not l2_reg:\n",
    "                    l2_reg = w.norm(2)\n",
    "                else:\n",
    "                    l2_reg = l2_reg + w.norm(2) \n",
    "            loss = loss_func(prediction, labels) + l2_reg * reg_lambda\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for iter, (batched_graph, offsets_bert, offsets_rgat, bert_embeddings, labels,features,train_types,neighbors) in enumerate(val_dataloarder):\n",
    "                offsets_rgat = offsets_rgat.cuda()\n",
    "                bert_embeddings = bert_embeddings.cuda()\n",
    "                labels = labels.cuda()\n",
    "                prediction = model(offsets_bert, offsets_rgat, bert_embeddings, features,train_types,neighbors)\n",
    "                loss = loss_func(prediction, labels)\n",
    "                val_loss += loss.detach().item()\n",
    "            val_loss = val_loss/(iter + 1)\n",
    "            \n",
    "            \n",
    "        if epoch%20 == 0:\n",
    "            print('Epoch {}, val_loss {:.4f}'.format(epoch, val_loss))\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if epoch > 20:\n",
    "                torch.save(model.state_dict(), 'best_model.pth') \n",
    "            if epoch > 36: print('Best val loss found: ', best_val_loss)\n",
    "\n",
    "    \n",
    "    print('This fold, the best val loss is: ', best_val_loss)\n",
    "    \n",
    "    test_loss = 0.\n",
    "    test_predict = None\n",
    "    \n",
    "    model = GPRModel().to(\"cuda:0\")\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for iter, (batched_graph, offsets_bert, offsets_rgat, bert_embeddings, labels,features,train_types,neighbors) in enumerate(test_dataloarder):\n",
    "            \n",
    "            offsets_rgat = offsets_rgat.cuda()\n",
    "            bert_embeddings = bert_embeddings.cuda()\n",
    "            labels = labels.cuda()\n",
    "            prediction = model(offsets_bert, offsets_rgat, bert_embeddings, features,train_types,neighbors)\n",
    "            \n",
    "            if test_predict is None:\n",
    "                test_predict = prediction\n",
    "            else:\n",
    "                test_predict = torch.cat((test_predict, prediction), 0) \n",
    "            loss = loss_func(prediction, labels)\n",
    "            test_loss += loss.detach().item()\n",
    "    \n",
    "    test_loss /= (iter + 1)\n",
    "    print('This fold, the test loss is: ', test_loss)\n",
    "    test_predict_lst.append(test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_arr = [torch.softmax(pre.cpu(), -1).clamp(1e-4, 1-1e-4).numpy() for pre in test_predict_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_preds = np.mean(test_predict_arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8161887 , 0.12780455, 0.05600675],\n",
       "       [0.96323794, 0.01873819, 0.01802389],\n",
       "       [0.02798579, 0.9661549 , 0.00585947],\n",
       "       ...,\n",
       "       [0.5187353 , 0.40772533, 0.07353938],\n",
       "       [0.6908145 , 0.05375566, 0.2554298 ],\n",
       "       [0.03592736, 0.87267363, 0.09139912]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4912520516128279"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def extract_target(df):\n",
    "    df[\"Neither\"] = 0\n",
    "    df.loc[~(df['A-coref'] | df['B-coref']), \"Neither\"] = 1\n",
    "    df[\"target\"] = 0\n",
    "    df.loc[df['B-coref'] == 1, \"target\"] = 1\n",
    "    df.loc[df[\"Neither\"] == 1, \"target\"] = 2\n",
    "    return df\n",
    "test_df = extract_target(test_df)\n",
    "log_loss(test_df.target, final_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_df.target==2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_preds.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.828     0.874     0.850       874\n",
      "           1      0.833     0.846     0.840       925\n",
      "           2      0.745     0.507     0.604       201\n",
      "\n",
      "    accuracy                          0.825      2000\n",
      "   macro avg      0.802     0.743     0.765      2000\n",
      "weighted avg      0.822     0.825     0.821      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_df.target, final_test_preds.argmax(axis = 1),digits =3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.816189</td>\n",
       "      <td>0.127805</td>\n",
       "      <td>0.056007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.963238</td>\n",
       "      <td>0.018738</td>\n",
       "      <td>0.018024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.027986</td>\n",
       "      <td>0.966155</td>\n",
       "      <td>0.005859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.014713</td>\n",
       "      <td>0.732934</td>\n",
       "      <td>0.252352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.039329</td>\n",
       "      <td>0.953091</td>\n",
       "      <td>0.007580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         A         B   NEITHER\n",
       "0  development-1  0.816189  0.127805  0.056007\n",
       "1  development-2  0.963238  0.018738  0.018024\n",
       "2  development-3  0.027986  0.966155  0.005859\n",
       "3  development-4  0.014713  0.732934  0.252352\n",
       "4  development-5  0.039329  0.953091  0.007580"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = pd.DataFrame(final_test_preds, columns=[\"A\", \"B\", \"NEITHER\"])\n",
    "df_sub[\"ID\"] = test_df.ID\n",
    "df_sub = df_sub[['ID',\"A\", \"B\", \"NEITHER\"]]\n",
    "df_sub.to_csv(\"submission_RGAT_Model.csv\", index=False)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "246px",
    "width": "337px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
